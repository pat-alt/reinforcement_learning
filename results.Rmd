Figure 3 we can observe the performance of the different sampling methods describe in the previous sections.
First thing we notice is that the sampling methods that incorporate a discounting factor are the ones doing the best in the non stationary setting. The fact that the sampling method incorporates a discounting factor when estimating the expected reward of the arm results in old realizations being weighted less than recent observations. Or in other words, recent information has more relevance when estimating the expected reward of the arm. In the non stationary setting this is clearly something that a priori could achieves much better results given that for very past information the underlying Bernoulli distribution of the arm must have had a different $\p$. This intuition is confirmed on our experiment as shown in Figure 3, where the sampling methods that achieves a lowest commutative regret. 

Note that both Discounted UCB and Discounted Thompson, compared to UCB and Thompson, learn very fast the change in the underlying Bernuilly distribution, and threfore not accomulating much regret on these periods, while we observe the opposite in the non disocunted sampling mehotd. However, we can see that once the non discounted methods have found the right arm the pull this arm all the periods, this is not the case for the discounting samplings in which a bit of exploration is done, therefore not sticking with the best arm.

When looking into UCB we can observe that it is the best sampling method in the beginning, when the frist probsbility change kicks in, it reacts very fast as the confidence interval for arm 1 (the one with highest probability in the second period) is very loose and the difference of the best and second best probability in this second period is high. In the third period we see something very interesting, we can see that UCB is much lower in terms of reacting to the probability changes and findgin the optimal arm in this period, this is dues to the fact that in this periods all the arms have a similar chance of succeeding. Once it has finally sampled a lot, it has finally decreased the variance so much that the small difference on the expected reward of the arms are significant, it identifies the correct arm. But the changes in the underlying probailbiy does not allow a high exploitation. Interesteingly, the final change in probabiliies clearly has a winning arm and as consequence the sampling methods does not need a lot of resmplings to get to find the winnig arm. 

As mentioned in previous sections, Thompson sampling performs better than UCB in a sationaray setting, but as seen in the results it is no longer the case in the non-sataionary setting. The bayesian way for sampling arms does not provied the necessary felxibility to the method in order to correctly identidy and adapt its smapling behaviour to the change in the underlying expected rewards of the differnt arms. On the other hand, the simple and intuitive approch of UCB of considering the upper confiende bound given the current estimate of the variance (driven by the amount of times we have explored the arm) and its estimated expected reward, adapts much better to changes in the distribution and therefore achiveing much bettter cunmmulative regret in the non stationary setting. 

Finally, both softmax and epsilon first do very poorly as expected. Epsilon first only explores the first K periods and threfore is very sensitve the very random draws, and for the rest  of the periods it just exploist the best arm found in the first K periods, threfore not being able to deal with the non stationarty. In the case of softmax, we can see that choosing the arm with respect to proabilities that are proportional to the mean rewards does not achieves a good performance.

Note that the majority of this sampling method incorporate some hyperparameters and threfore they can be tuened in order to boost its performance. 
