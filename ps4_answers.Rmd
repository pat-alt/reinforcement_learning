---
title: "Problem Set 4"
subtitle: "Approximate Dynamic Programming"
author: "Patrick Altmeyer"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    code_folding: show
    number_sections: true
    toc: true
    toc_float: true
  bookdown::pdf_document2:
    number_sections: true
    toc: false
    includes:
      in_header: preamble.tex
      before_body: before_body.tex
bibliography: bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = FALSE, fig.align = "center", out.width = 500)
library(data.table)
library(ggplot2)
library(scales)
library(microbenchmark)
lapply(
  list.files("R", pattern = "\\.R"),
  function(file) {
    source(file = file.path("R",file))
  }
)
rand_seed <- 123
theme_set(theme_bw())
```

There are two fundamental challenges of reinforcement learning (RL):

1. Reward and transition functions are unknown.
2. The state and action space are large.

Approximate Dynamic Programming deals with both challenges. 

# Approximate Policy Evaluation

First we will redefine the Markov Decision Process from [Problem Set 1](ps1_answers.html). This time we will not provide the entire state and action space, but instead rely solely on the reward and transition functions to define the MDP.

```{r}
mdp <- readRDS("results/mdp.rds")
```

Next we code up the different feature maps:

```{r}
# Fine map:
phi_fine <- function(x, state_space) {
  phi <- as.numeric(0:length(state_space)==x)
  return(phi)
}
# Coarse map:
phi_coarse <- function(x, state_space, width=5) {
  idx <- 1:((length(state_space))/width)
  bounds <- data.table(lb=width*(idx-1),ub=width*idx-1)
  phi <- bounds[,as.numeric(x %between% .(lb,ub))]
  return(phi)
}
# Piece-wise map:
phi_pwl <- function(x, state_space, width=5) {
  # Coarse part:
  idx <- 1:((length(state_space))/width)
  bounds <- data.table(lb=width*(idx-1),ub=width*idx-1)
  phi_coarse <- bounds[,as.numeric(x %between% .(lb,ub))]
  # Other part:
  phi_other <- phi_coarse * (x - width*(idx - 1)) / width
  phi <- c(phi_coarse, phi_other)
  return(phi)
}
# List:
feature_maps <- list(
  fine = phi_fine,
  coarse = phi_coarse,
  pwl = phi_pwl
)
```

Finally we add to the `mdp` instance the `width` argument, which will be used by the feature maps below.

```{r}
mdp$width <- 5
```

While in [Problem Set 1](ps1_answers.html) we supplied a function to the `mdp` instance that would be used to compute a matrix describing the probabilities of transitioning from one state to another, here we will need to supply a function that actually generates a random transition based on the transition function.

```{r}
transition_fun <- function(state, action, state_space, p) {
  x <- state + rbinom(1,1,p) - rbinom(1,1,action)
  new_state <- min(max(state_space), max(x,0))
  return(new_state)
}
mdp$transition_fun <- transition_fun
```

As before we introduce the two deterministic policies:

```{r, echo=TRUE}
# Policies:
lazy <- function(state, action_space) {
  action <- rep(action_space[1], length(state))
  return(action)
}

aggressive <- function(state, action_space) {
  action <- ifelse(state<50, action_space[1], action_space[2])
  return(action)
}
```

```{r, echo=TRUE}
# Lazy:
policy_lazy <- lazy(state = mdp$state_space, action_space = mdp$action_space)
# Aggressive:
policy_aggr <- aggressive(state = mdp$state_space, action_space = mdp$action_space)
policies <- list(
  lazy = policy_lazy,
  aggr = policy_aggr
)
```

```{r, eval=FALSE}
# Set up the grid:
sigma <- 1e-5
n_iter <- c(1e3,1e4,1e5)
grid <- CJ(n_iter=n_iter, feature_map=names(feature_maps), policy=names(policies))
set.seed(42)
trajectories <- list(
  lazy = sim_trajectory(mdp, policies[["lazy"]], n_iter = max(n_iter)),
  aggr = sim_trajectory(mdp, policies[["aggr"]], n_iter = max(n_iter))
)
sim_output <- rbindlist(
  lapply(
    1:nrow(grid),
    function(i) {
      list2env(c(grid[i,]), envir = environment())
      message(
        sprintf(
          "Running: feature map: %s, policy: %s, n_iter: %i", 
          feature_map, policy, n_iter
        )
      )
      mdp$feature_map_fun <- feature_maps[[feature_map]]
      traj_temp <- trajectories[[policy]][1:n_iter,]
      out_td <- td(mdp, traj_temp)
      V_td <- out_td$V
      out_lstd <- lstd(mdp, traj_temp, sigma=sigma)
      V_lstd <- out_lstd$V
      output <- data.table(
        td = V_td,
        lstd = V_lstd,
        n_iter = n_iter,
        policy = policy,
        feature_map = feature_map
      )
      return(output)
    }
  )
)
sim_output <- melt(sim_output, measure.vars = c("td", "lstd"))
sim_output[,state:=mdp$state_space,by=.(policy, feature_map, variable, n_iter)]
saveRDS(sim_output, file="results/ps4_sim_reg.rds")
```

```{r, eval=FALSE}
regularization <- 1e-30 
sim_output <- rbindlist(
  lapply(
    1:nrow(grid),
    function(i) {
      list2env(c(grid[i,]), envir = environment())
      message(
        sprintf(
          "Running: feature map: %s, policy: %s, n_iter: %i", 
          feature_map, policy, n_iter
        )
      )
      mdp$feature_map_fun <- feature_maps[[feature_map]]
      traj_temp <- trajectories[[policy]][1:n_iter,]
      out_td <- td(mdp, traj_temp)
      V_td <- out_td$V
      out_lstd <- lstd(mdp, traj_temp, sigma = regularization)
      V_lstd <- out_lstd$V
      output <- data.table(
        td = V_td,
        lstd = V_lstd,
        n_iter = n_iter,
        policy = policy,
        feature_map = feature_map
      )
      return(output)
    }
  )
)
sim_output <- melt(sim_output, measure.vars = c("td", "lstd"))
sim_output[,state:=mdp$state_space,by=.(policy, feature_map, variable, n_iter)]
saveRDS(sim_output, file="results/ps4_sim.rds")
```


```{r}
sim_output_reg <- readRDS("results/ps4_sim_reg.rds")
sim_output <- readRDS("results/ps4_sim.rds")
```

Found that higher levels of regularization led to poorer estimates.

```{r}
p <- ggplot(data=sim_output[policy=="aggr"], aes(x=state, y=value, linetype=variable)) +
  geom_line() +
  facet_grid(
    rows=vars(feature_map),
    cols=vars(n_iter)
  )
p
```

```{r}
p <- ggplot(data=sim_output[policy=="lazy"], aes(x=state, y=value, linetype=variable)) +
  geom_line() +
  facet_grid(
    rows=vars(feature_map),
    cols=vars(n_iter)
  )
p
```

# Approximate Policy Iteration

```{r}
n_iter <- c(10,100)
png("www/optimal_polciy_approx.png", width = 900, height = 500)
par(mfrow=c(1,length(n_iter)))
mdp$feature_map_fun <- phi_pwl
policy_iter_results <- list()
for (i in 1:length(n_iter)) {
  policy_iter_results[[i]] <- appr_policy_iteration(
    mdp,
    n_iter = n_iter[i], 
    n_trans = 1e4, 
    verbose = 1,
    sigma = 1e-30
  )
}
saveRDS(policy_iter_results, file="results/optimal_policy_approx.rds")
dev.off()
```

```{r}
optimal_policy <- readRDS(file="results/optimal_policy_approx.rds")
```


