---
title: "Piece-wise stationary multi-armed bandits"
subtitle: "Final project"
author: 
- "Marc Agust√≠ (marc.agusti@barcelonagse.eu)"
- "Patrick Altmeyer (patrick.altmeyer@barcelonagse.eu)"
- "Ignacio Vidal-Quadras Costa (ignacio.vidalquadrascosta@barcelonagse.eu)"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::pdf_document2:
    number_sections: true
    toc: false
    includes:
      in_header: preamble.tex
      before_body: before_body.tex
  bookdown::html_document2:
    code_folding: show
    number_sections: true
    toc: true
    toc_float: true
fontsize: 12pt
bibliography: bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(
  echo = FALSE, 
  fig.align = "center"
)
library(reticulate)
library(ggplot2)
library(data.table)
```

```{python setup-py}
# Libraries:
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
from numpy import log
from numpy import random
from numpy import sqrt
from numpy import argmax
from numpy import zeros

# Functions:
from python.mab import *
```



@raj2017taming - **Taming Non-stationary Bandits: A Bayesian Approach**
@besbes2014stochastic - **Stochastic multi-armed-bandit problem with non-stationary rewards**
@gupta2011thompson - **Thompson Sampling for Dynamic Multi-armed Bandits**
@garivier2008upper - **On upper-confidence bound policies for non-stationary bandit problems**

\pagebreak

# Introduction

The multi-armed bandit (MAB) is a problem in reinforcement learning that focuses on how to solve the exploration-exploitation dilemma [@sutton2018reinforcement]. Each of the arms has a probability of succeeding which is modelled by a Bernoulli distribution with a parameter $p$. Most of the theory around multi-armed bandits covered in class and its respective implementations assume stationary on the arms, that is, the probability of an arm succeeding does not change through time. However, in most real life settings, this strong assumption is not satisfied [@raj2017taming]. 

For instance, consider the problem deciding which news to put in the front page of a news paper that will capture the attention of as many readers as possible. In order to model the response of the reader to the news shown, one can use a Bernoulli distribution where the $p$ describes the probability that the user clicks on the news link. In the stationary setup, this probability is assumed to be constant, which is unrealistic: there are trends that lead to some articles being more popular during some period and less popular during other times. For instance, during the Eurocup, an article on football can be predicted to have a lot of clicks, however once the Eurocup is over and friendly games take over, an article on football might not be as interesting anymore and thus getting fewer clicks.

To this end, in this project we explore different strategies that have been proposed and tested in order to deal with the complication of non-stationarity. We compare the different strategies empirically. The remainder of this note is structured as follows: in section \@ref(strat) we briefly summarise a set of recent papers that have emerged from this line of literature. This will provide us with a set of difference strategies for solving non-stationary multi-armed bandits and serve as the foundation for an empirical investigation of their performance in section \@ref(emp). Finally, in section \@ref(dis) we discuss the empirical results and conclude.

# Strategies for solving non-stationary MABs {#strat}

```{python}
horizon = 10000
def bandits(t):
    # With this function we will create the true underlying 
    # probabilities for all the bandits 
    if t < 2000:
        vector_prob = [0.2,0.5,0.8]
    elif 2000 <= t <= 4999:
        vector_prob = [0.95,0.7,0.3]
    elif 5000 <= t <= 5999:
        vector_prob = [0.05,0.65,0.35]
    else:
        vector_prob = [0.5,0.6,0.95]
    return vector_prob

probs = np.asarray([bandits(t) for t in range(horizon)])
```

Before diving into non-stationarity we briefly review a few of the conventional approaches that are used to solve stationary multi-armed bandits. Consider a multi-armed bandit with $K$ all of which yield rewards that are generated by a Bernoulli distribution. Let $\mathbf{p}$ denote the vector of constant (stationary) Bernoulli probabilities where element $p_k$, $k=1,...,K$ indicates the unconditional probability that arm $k$ yields a reward. Without loss of generality we will assume that rewards in period $t$ are binary: $\mathbf{r}_t=[0,1]^{K}$.

```{r unique-probs}
probs <- data.table(py$probs)
setnames(probs, names(probs), sprintf("arm_%i", 1:ncol(probs)))
time_passed <- probs[,(max(.I)+1)-min(.I),by=paste(arm_1,arm_2,arm_3)]$V1
unique_probs <- unique(probs)
unique_probs[,period:=1:.N]
unique_probs[,time_passed:=time_passed]
setcolorder(unique_probs, c("period","time_passed"))

unique_probs_l <- melt(unique_probs, id.vars = c("period", "time_passed"))
beta <- function(x, a, b) {
  numerator <- x^(a-1) * (1-x)^(b-1)
  gamma_a <- factorial(a-1)
  gamma_b <- factorial(b-1)
  gamma_ab <- factorial(a+b-1)
  denominator <- (gamma_a * gamma_b)/gamma_ab
  return(numerator/denominator)
}
posteriours <- unique_probs_l[
  ,
  .(
    x=seq(0,1,0.01),
    beta=beta(
      x=seq(0,1,0.01),
      a=value*10,
      b=(1-value)*10
    )
  ),
  by=.(period,variable)
]
```

```{r, fig.height=3, fig.width=9}
ggplot(data=posteriours[period==1], aes(x=x, y=beta, fill=variable, colour=variable)) +
  geom_area(alpha=0.25, show.legend = FALSE) +
  facet_grid(
    cols=vars(variable),
    rows=vars(period)
  ) +
  labs(
    x="Estimated Action Value",
    y="Density"
  ) 
```

```{r, fig.height=9, fig.width=9}
ggplot(data=posteriours, aes(x=x, y=beta, fill=variable, colour=variable)) +
  geom_area(alpha=0.25, show.legend = FALSE) +
  facet_grid(
    cols=vars(variable),
    rows=vars(period)
  ) +
  labs(
    x="Estimated Action Value",
    y="Density"
  ) +
  coord_cartesian(ylim=c(0,5))
```

The **Upper Confidence Bound** (UCB) approach to solving the multi-armed bandit problem involves ... [@sutton2018reinforcement]
**Thompson Sampling** has been shown to outperform UCB in the context of stationary multi-armed bandits [@chapelle2011empirical].

# Empirical investigation {#emp}

# Discussion {#dis}

\FloatBarrier
\pagebreak

# References {-}

<div id="refs"></div>



