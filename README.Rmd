---
output: github_document
bibliography: bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  out.width = 500,
  fig.align = "center"
)
```

## Dynamic Programming



## Multi-Armed Bandit problems

[Problem Set 2](ps2_answers.html) replicates an empirical evaluation of Thompson Sampling [@chapelle2011empirical]. Code for this problem has been implemented in R and performance-enhanced through `Rcpp` (`C++`).

To run the simulation from the command line, simply execute the command below. This will clone the git repo to your device and run the simulation with parameters specified in `run_simulation`.

```{bash, eval=F}
git clone https://github.com/pat-alt/reinforcement_learning.git
cd reinforcement_learning
Rscript -e 'source("requirements.R"); run_simulation(n_sim=5,horizon=1e6,update_every = 100)'  
open 'www/user_sim.png'
```

With the given set of parameters the computations should only take a few minutes and the resulting chart should looks something like this:

```{r}
knitr::include_graphics("www/user_sim.png")
```

The results from the full simulation are shown below. All details and documentation can be found in the [HTML document](ps2_answers.html).

```{r}
knitr::include_graphics("www/ps1_sim.png")
```

## Gaussian Processes

The first part of [Problem Set 3](ps3_answers.html) is about Gaussian Process Regression. The plot below shows the output from a Gaussian Process Regression with 20 training points.

```{r}
knitr::include_graphics("www/gp_reg.png")
```



## Bayesian Optimization

The Bayes Optimizer gradually gets better at estimated the true function values. As it explores different points on the test grid uncertainty around these points shrinks. Sometimes the overall magnitude of the confidence interval suddenly appears to change which corresponds to occasions when the estimates of optimal hyperparameters change significantly. Eventually the learned function values are very close to true function values and the proposed optimum corresponds to the true optimum (among the test points).

```{r}
knitr::include_graphics("www/bayes_opt.gif")
```

The Figure below shows the loss evolution with different acquisition functions for varying exploration parameters (by row). For UCB and EI the results are intuitive with the former performing better overall: as the exploration rate increases they are more likely to explore and hence at times commit errors. For PI the results are poor across the board. The choices of the eploration rate may be too high for PI. Another explanation may be that due to computational constraints, I have run the multi-started hyperparameter optimization only every five iterations. 

```{r benchmark, fig.cap="Comparison of loss evolution with different acquisition functions for varying exploration parameters (by row)."}
library(data.table)
library(ggplot2)
sim_output <- readRDS("results/bayes_opt_sim.rds")
sim_output <- rbindlist(
  lapply(
    sim_output,
    function(i) {
      loss <- data.table(
        iter = 1:length(c(i$output$loss)),
        loss = c(i$output$loss),
        explore = i$exploring_rate,
        acqui = i$acqui_fun
      )
      return(loss)
    }
  )
)
p <- ggplot(data=sim_output, aes(x=iter, y=loss, colour=acqui)) +
  geom_line() +
  facet_grid(rows = vars(explore)) +
  scale_color_discrete(
    name="Acquisition function:"
  ) +
  labs(
    x="Iteration",
    y="Loss"
  )
p
```

## References

<div id="refs"></div>

## Session Info

```{r}
utils::sessionInfo()
```


