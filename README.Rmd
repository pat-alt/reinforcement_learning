---
output: github_document
bibliography: bib.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  out.width = 500,
  fig.align = "center"
)
library(data.table)
library(ggplot2)
theme_set(theme_bw())
```

> NOTE: Exercises were either proposed or designed by the course instructores, [Gergely Neu](http://cs.bme.hu/~gergo/) and [Hrvoje Stojic](https://hstojic.re/).

## Dynamic Programming

[Problem Set 1](ps1_answers.html) is about implementing policy evaluation and iteration methods for a simple Markov Decision Process. Implementation in R closely follows @sutton2018reinforcement. The chart below demonstrates how the optimal policy and value function is gradually learned through value iteration.

```{r val-it}
knitr::include_graphics("www/value_iteration.png")
```

## Multi-Armed Bandit problems

[Problem Set 2](ps2_answers.html) replicates an empirical evaluation of Thompson Sampling [@chapelle2011empirical]. Code for this problem has been implemented in R and performance-enhanced through `Rcpp` (`C++`).

To run the simulation from the command line, simply execute the command below. This will clone the git repo to your device and run the simulation with parameters specified in `run_simulation`.

```{bash, eval=F}
git clone https://github.com/pat-alt/reinforcement_learning.git
cd reinforcement_learning
Rscript -e 'source("requirements.R"); run_simulation(n_sim=5,horizon=1e6,update_every = 100)'  
open 'www/user_sim.png'
```

With the given set of parameters the computations should only take a few minutes and the resulting chart should looks something like this:

```{r}
knitr::include_graphics("www/user_sim.png")
```

The results from the full simulation are shown below. All details and documentation can be found in the [HTML document](ps2_answers.html).

```{r}
knitr::include_graphics("www/ps1_sim.png")
```

## Gaussian Processes

The first part of [Problem Set 3](ps3_answers.html) is about Gaussian Process Regression. The plot below shows the output from a Gaussian Process Regression with 20 training points.

```{r}
knitr::include_graphics("www/gp_reg.png")
```



## Bayesian Optimization

The Bayes Optimizer gradually gets better at estimated the true function values. As it explores different points on the test grid uncertainty around these points shrinks. Sometimes the overall magnitude of the confidence interval suddenly appears to change which corresponds to occasions when the estimates of optimal hyperparameters change significantly. Eventually the learned function values are very close to true function values and the proposed optimum corresponds to the true optimum (among the test points).

```{r}
knitr::include_graphics("www/bayes_opt.gif")
```

The Figure below shows the loss evolution with different acquisition functions for varying exploration parameters (by row). For UCB and EI the results are intuitive with the former performing better overall: as the exploration rate increases they are more likely to explore and hence at times commit errors. For PI the results are poor across the board. The choices of the eploration rate may be too high for PI. Another explanation may be that due to computational constraints, I have run the multi-started hyperparameter optimization only every five iterations. 

```{r benchmark, fig.cap="Comparison of loss evolution with different acquisition functions for varying exploration parameters (by row).", echo=FALSE}
sim_output <- readRDS("results/bayes_opt_sim.rds")
sim_output <- rbindlist(
  lapply(
    sim_output,
    function(i) {
      loss <- data.table(
        iter = 1:length(c(i$output$loss)),
        loss = c(i$output$loss),
        explore = i$exploring_rate,
        acqui = i$acqui_fun
      )
      return(loss)
    }
  )
)
p <- ggplot(data=sim_output, aes(x=iter, y=loss, colour=acqui)) +
  geom_line() +
  facet_grid(rows = vars(explore)) +
  scale_color_discrete(
    name="Acquisition function:"
  ) +
  labs(
    x="Iteration",
    y="Loss"
  )
p
```

## Approximate Dynamic Programming

There are two fundamental challenges of reinforcement learning (RL):

1. Reward and transition functions are unknown.
2. The state and action space are large.

Approximate Dynamic Programming deals with both challenges.

The chart below shows the approximate evaluations of the two deterministic policies for different numbers of sample transitions (across columns) and different feature maps (across rows). Broadly speaking the estimates tend to be less noisy as the number of sample transitions increases.

```{r, echo=FALSE}
sim_output <- readRDS("results/ps4_sim.rds")
p <- ggplot(data=sim_output[policy=="lazy"], aes(x=state, y=value, linetype=variable)) +
  geom_line() +
  facet_grid(
    rows=vars(feature_map),
    cols=vars(n_iter)
  )
p + 
  labs(title="Lazy policy, barely regularized")
```

Approximate policy iteration is noisy for high states. Nonetheless the proposed policies are close two optimal:

```{r, echo=FALSE}
optimal_policy <- readRDS(file="results/optimal_policy_approx.rds")
value_iter_results <- readRDS("results/value_iter.rds")
val_it <- value_iter_results[[4]]
N <- length(val_it$policy)
proposed_policies <- rbind(
  data.table(state=1:N, action=val_it$policy,type="optimal"),
  data.table(state=1:N, action=optimal_policy[[1]]$policy,type="few_iter"),
  data.table(state=1:N, action=optimal_policy[[2]]$policy,type="many_iter")
)
ggplot(data=proposed_policies, aes(x=state, y=action)) +
  geom_point(cex=.25, colour="blue") +
  facet_grid(
    rows=vars(type)
  )
```


## References

<div id="refs"></div>

## Session Info

```{r}
utils::sessionInfo()
```


