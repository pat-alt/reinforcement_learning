---
title: "Problem Set 1"
subtitle: "Dynamic Programming"
author: "Patrick Altmeyer"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output: 
  bookdown::html_document2:
    code_folding: hide
    number_sections: true
    toc: true
    toc_float: true
bibliography: bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, fig.align = "center")
library(data.table)
library(ggplot2)
library(scales)
lapply(
  list.files("R", pattern = "\\.R"),
  function(file) {
    source(file = file.path("R",file))
  }
)
rand_seed <- 123
theme_set(theme_bw())
```

# Policy evaluation

```{r}
N <- 100
# State space:
state_space <- seq(0,N-1,by=1)
# Action space:
action_space <- c(0.51, 0.6)
# Reward function:
reward_fun <- function(state, action, state_space) {
  reward <-  (-1) * ((state/max(state_space+1))^2 + ifelse(action==0.51,0,0.01))
}
# Transition function:
transition_fun <- function(new_state,state,action, p) {
  
  # Standard rule:
  state_diff <- data.table(new_state = new_state, state = state, q=action)
  state_diff[,diff:=new_state-state]
  state_diff[,prob:=0] # initalize all as zero
  state_diff[diff==1,prob:=p*(1-q)]
  state_diff[diff==0,prob:=p*q + (1-p)*(1-q)]
  state_diff[diff==-1,prob:=(1-p)*q]
  
  # Boundary cases:
  state_diff[state==min(state) & new_state==min(state),prob:=1 - p * (1-q)]
  state_diff[state==max(state) & new_state==max(state),prob:=1 - (1-p) * q]
  
  return(state_diff$prob)
}
# Discount factor:
discount_factor <- .9
```

```{r}
# Policies:
lazy <- function(state, action_space) {
  action <- rep(action_space[1], length(state))
  return(action)
}

aggressive <- function(state, action_space) {
  action <- ifelse(state<50, action_space[1], action_space[2])
  return(action)
}
```


```{r}
p <- 0.5
mdp <- define_mdp(
  state_space = state_space,
  action_space = action_space,
  reward_fun = reward_fun,
  transition_fun = transition_fun,
  discount_factor = discount_factor,
  p=p
)
```

```{r}
# Lazy:
policy_lazy <- lazy(state = mdp$state_space, action_space = mdp$action_space)
V_pi_lazy <- evaluate_policy(mdp, policy = policy_lazy)
# Aggressive:
policy_aggr <- aggressive(state = mdp$state_space, action_space = mdp$action_space)
V_pi_aggr <- evaluate_policy(mdp, policy = policy_aggr)
```

```{r}
valuations <- data.table(state=state_space, lazy=V_pi_lazy, aggr=V_pi_aggr)
dt_plot <- melt(valuations, id.vars = "state")
ggplot(dt_plot, aes(x=state, y=value, colour=variable)) +
  geom_hline(aes(yintercept = 0), size=0.25) +
  geom_line() +
  labs(
    x="State",
    y="Value"
  )
```

```{r}
dt_plot <- data.table(state=1:length(state_space), value=V_pi_lazy-V_pi_aggr)
ggplot(dt_plot, aes(x=state, y=value)) +
  geom_line() +
  geom_hline(aes(yintercept = 0), size=0.25) +
  labs(
    x="State",
    y="Difference"
  )
```

# Policy iteration

```{r, eval=FALSE, include=FALSE}
png("www/policy_iteration.png", width = 600, height = 600)
max_iter <- c(10,20,50,100)
par(mfrow=rep(sqrt(length(max_iter)),2))
policy_iter_results <- list()
for (i in 1:length(max_iter)) {
  policy_iter_results[[i]] <- policy_iteration(
    mdp = mdp,
    max_iter = max_iter[i],
    verbose = 1 # to produce plot 
  )
}
dev.off()
```

```{r pol-it, fig.cap="Policy iteration after 10, 20, 50 and 100 iterations. The blue line represents the value function corresponding to the final estimate of the optimal value function."}
knitr::include_graphics("www/policy_iteration.png")
```


# Value iteration



